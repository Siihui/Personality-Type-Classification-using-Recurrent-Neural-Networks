{"cells":[{"cell_type":"markdown","metadata":{"id":"A7v4GVxo4Dom"},"source":["# 1 - Data Preprocessing"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HftyG77k47Y3"},"source":["## 1.0. Data Collection "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35082,"status":"ok","timestamp":1650164923769,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"U7C4snIcNl22","outputId":"53081102-d012-46e5-d46d-947a759e68bf","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["------------------------------------\n","Size of training dataset: 7808\n","Size of testing dataset: 867\n","------------------------------------\n","------------------------------------\n","Sample Data\n","LABEL: F / SENTENCE: 'Half of it is going straight to charity, another quarter going straight to scientific research, an eighth to the parkour community, a sixteenth to towards spreading information about health and...|||Find a path or suffer more.|||http://personalitycafe.com/enneagram-personality-theory-forum/85323-enneagram-type-mbti-type-compared-statistics.html yep.|||I kind of anchor on Fi and Ne makes having Ni really fun. INFP for me as they tire me out less and our views tend to align more.|||The two ESTPs I have gotten the chance to know seem to experience much more than other people who have been on the planet for the same amount of time and are quite the renaissance (wo)men.  Is this...|||I don't really have a best friend ISTP(passion-amateur group co-founder), INTJ(intellectual and various small hobbies talk), ESTP(Bro-in-law, talk about everything kind of like my INTJ friend),...|||Everyone looses their gift if they don't even consider a different perspective.|||Kansas - ISTJ|||That or if they are normally comfortable with me, such as a friend or close acquaintance, they feel the need to start talking. It's almost a trap, I've noticed for most people feel the need to expose...|||To me, your answers screamed introverted feeling. Answers 2-5, 10, 11, 14, 16, and 17 your last statement were particularly Fi-like. I'm guessing you are an intuitive and possibly and introvert...|||Could you explain your reasoning for these? I saw Mako as an F, Lin as an ES, and have Kya as an F. Never had an idea for Amon's type.|||This applies to many of these threads.|||With an INFP for over 2 years now.|||After watching tonight's episode I'm sure that Unalaq is an ENXJ. I'm not sure if it's Fe or Te at this point but the way he goes about doing and planning things seem like a Je-dom. I'm putting him...|||Parkour is my passion(but I consider it closer to a martial art than a sport). I also enjoy some running and climbing.|||I have many characters but I gravitate towards sneaky archer, Breton, and conjuration. I love doing role plays and think it's one of, if not the best way to play the game.|||ESFP seems right for Ikki. We may need Jinora to have more interactions for us to tell. Any guesses about Pema, Tenzin's wife? She said herself that she used to be very shy so I'd put I just from...|||If you don't mind, please tell me more by what you meant by this bolded part or what happened.|||I think it's fit to revive this thread seeing as the second season of Korra has started and the second episode of the season is coming up tomorrow. I'd just say beware of spoilers in new posts if you...|||I was thinking more along these lines: 83385|||Yes, a few times in friendships and other things but it was usually spurred on by the idea of not having a second chance. I've been trying to make the first move more in life as I've realized it just...|||Sorry if my wording was/is confusing or vague. Let me try to explain it better.  As for the first statement: I see the world for all it's interconnections. If you wish, visualized everything having...|||~I don't experience it as simply perceiving or creating, for me as I perceive interconnected relationships are formed and realized.   ~I don't think that I rationalize with my dominate function but...|||I think it's amusing that, in the leading position I share with an ISTP friend of mine, we both start to embrace our shadows. I Think that's been my growing point lately, embracing my shadow. We're...|||I would suggest introspection and relying on your sense of self over tests and I highly suggest looking into the cognitive functions. ISTJ is the complete opposite of INFJ.|||I definitely agree with others on the US- It's pretty good for an INFJ if you find your niche.  I say the Midwest is generally SJ with women expected to be F and men to be T. It's nice but annoying....|||Please explain|||I think my own eye movements have almost been changed because of where I was usually placed when talking to someone in normal conversations. See, when I was young I ended up getting permanent spot in...|||Judgmental, critical, somewhat narcissistic, stubborn, possessive, Fe-ishly manipulative, and I have ego issues. Take that with a grain of salt.|||Yes, very much so. I love Spanish so far.|||I have a huge folder of these types of images.|||Aquarian It was just my guess, it doesn't need that much merit. Personally, I think Se is the hardest function to describe because it is so in the moment.|||Sorry, double post because of connectivity weirdness.|||I don't know if this has been posted before or if a thread about curses would be the best place but it'll do just fine. The important part is post #79, the giant wall of text. I think most of it was...|||If anything, imo, Ni would be how objects are interconnected. If I were to follow closely to your model: Introverted Intuition: Understanding how objects are connected Extraverted Intuition:...|||Sometimes you just don't see them :ninja: Seriously, I thought I was alone in a small town but I was surprised after training for a couple months.  You can easily learn and train by yourself, you...|||82063 Stuff by Andy Day, not only do I like it because it is the stuff of my passion but that new perspective of our surroundings that it brings. This is a great example of that. All those people...|||Sorry for the quality, my relative only gave me a physical copy, it's a picture of a picture. This is my INFP girlfriend of two years and me.|||If I am with my SO I almost need physical contact in some way.|||I pretty much have a guru dream that involves my SP wannabe passion. Around people I am close to I totally put on the gypsy king face, people are just so interesting. Hahaha can't stop laughing at ...|||I agree this this post very much, I just can't shake that vibe. To me it feels like you are an INTP who strongly identifies with INFJs. I think if you want a sound answer form yourself and others we...|||I do pretty well in emergencies, I do very well compared to normal conditions in my opinion. I feel like I become the ideal version of myself, for the most part. It's hard to describe but it's like...|||I have a very close INTJ friend. The Te Fe difference is acknowledged very well and I'd say that both of our tertiary functions are well developed which helps a ton. He does not show it often but he...|||Being alone and/or doing something physical that I can naturally and reactively do without thinking or little thought.|||Pretty much this|||If I wear shoes or socks to bed and my feet are not on my bed I will wake up as if I was falling. 2/3 of the time this happens. Any other dreams that I remember(I don't remember most of my dreams...|||This one still gets me.  What I meant to say was Pass the salt but what I really said was You b****, you ruined my life|||I'm sorry, but I find them so funny because I use them for good reason. They make people uncomfortable at first but then, slowly, make people more comfortable with the idea that people are different...|||XSFJ Mother, ISTJ father, and an XNFJ sister. Yep.|||I love dark jokes, especially racists/stereotypical jokes.'\n","------------------------------------\n"]}],"source":["# Code to download file into Colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","id = ' '\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('training_data.csv')  \n","\n","id = ' '\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('testing_data.csv')  \n","\n","import pandas as pd\n","training_data = pd.read_csv(\"/content/training_data.csv\")\n","testing_data = pd.read_csv(\"/content/testing_data.csv\")\n","\n","print(\"------------------------------------\")\n","print(\"Size of training dataset: {0}\".format(len(training_data)))\n","print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n","print(\"------------------------------------\")\n","\n","print(\"------------------------------------\")\n","print(\"Sample Data\")\n","print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data.iloc[-1,0], training_data.iloc[-1,1]))\n","print(\"------------------------------------\")\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import gensim.downloader as api\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords as sw\n","from nltk.stem.porter import *\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import FastText\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=Warning)\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import re\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1650171340855,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"WIHEbmSQp3fh","outputId":"386e1cf4-6a53-4bb0-e6fa-18fff4c66b6e","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-3130623e-f016-45f4-a2d2-5f5112f3c3a0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>F</td>\n","      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>T</td>\n","      <td>'I'm finding the lack of me in these posts ver...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>T</td>\n","      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>T</td>\n","      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>T</td>\n","      <td>'You're fired.|||That's another silly misconce...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3130623e-f016-45f4-a2d2-5f5112f3c3a0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3130623e-f016-45f4-a2d2-5f5112f3c3a0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3130623e-f016-45f4-a2d2-5f5112f3c3a0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  type                                              posts\n","0    F  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n","1    T  'I'm finding the lack of me in these posts ver...\n","2    T  'Good one  _____   https://www.youtube.com/wat...\n","3    T  'Dear INTP,   I enjoyed our conversation the o...\n","4    T  'You're fired.|||That's another silly misconce..."]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# (1)type - label of the post (2)posts - the corresponding post content\n","training_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SvGBOm9DvR_","vscode":{"languageId":"python"}},"outputs":[],"source":["# Extract the labels and posts and store into List\n","\n","# Get the list of training data (posts)\n","training_posts=training_data['posts'].tolist()\n","# Get the list of corresponding labels for the training data (posts)\n","training_labels=training_data['type'].tolist()\n","# Get the list of testing data (posts)\n","testing_posts=testing_data['posts'].tolist()\n","# Get the list of corresponding labels for the testing data (posts)\n","testing_labels=testing_data['type'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEdxqO1qHfjZ","vscode":{"languageId":"python"}},"outputs":[],"source":["training_posts[1:5]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"l9gBSgBCQh24"},"source":["## 1.1. URL Removal\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emyl1lWxGr12","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","# Process training data\n","training_posts_remove_url=[]\n","for sentense in training_posts:\n","  sentense = re.sub(pattern, '', sentense)\n","  training_posts_remove_url.append(sentense)\n","\n","# Process test data\n","test_posts_remove_url=[]\n","for sentense in testing_posts:\n","  sentense = re.sub(pattern, '', sentense)\n","  test_posts_remove_url.append(sentense)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QzLAO5a25qzS"},"source":["## 1.2. Preprocess data (e.g. Stop words, Stemming)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1067354,"status":"ok","timestamp":1650088269072,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"jl7t5Vqo5_gq","outputId":"15d41385-5f85-4e21-ff68-4015238d90fa","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["\n","# Process training data\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  # Remove url\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # lower \n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  #remove stop words\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  # Stemming\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  # Lemmatisation\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","# Process test data\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  # Remove url\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # Lower\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  # Remove stop words\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  #stemming\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  # Lemmatisation\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)"]},{"cell_type":"markdown","metadata":{"id":"6sAZNIg5927R"},"source":["# 2 - Input Representation\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"daDvAftceIvr"},"source":["## 2.1. Word Embedding Construction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jw8I1QBk-EhG","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","# In practice, we only use the vocabulary from training data\n","# training data + test data\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","# create look up table for dataset\n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","\n","# Word embedding ussing FastText(SG), window sizes = 100\n","# use both training and test data\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_dim = ft_sg_model.vector_size #200\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_table.append(ft_sg_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)  \n","\n","\n","# Label encoding\n","# training set\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","\n","unique_labels = np.unique(training_labels)\n","\n","# test set\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LNys5HOdISK-"},"source":["## 2.2. Pretrained Word Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ae8i7Z2kIef-","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","# Pretrained word embedding: glove-twitter-200\n","# download the model and return as object ready for use\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T0ap96aeGlIk"},"source":["## 2.3. Input Concatenation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2CUCL1cGlI2","vscode":{"languageId":"python"}},"outputs":[],"source":["# Concatenation\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*ft_sg_model.vector_size),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Set total length to 700\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)"]},{"cell_type":"markdown","metadata":{"id":"LIu_lkJwQ55g"},"source":["# 3 - Model Implementation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DpYCL17JKZxl"},"source":["### 3.1. Build Sequence Model (Bi-directional model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpGfp9FJF__S","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device('cpu')\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False)\n","\n","vocab_size = len(word_list)\n","total_epoch = 125\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6BaOiaGRLW7R"},"source":["### 3.2. Train Sequence Model (Bi-directional model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77843,"status":"ok","timestamp":1650089737485,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"lVQnUSX1LZ6C","outputId":"b9eac034-6e86-4949-e6c9-6fef3fcda49f","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, loss: 0.72491, train_acc: 0.54\n","Epoch: 2, loss: 0.76567, train_acc: 0.47\n","Epoch: 3, loss: 0.70835, train_acc: 0.59\n","Epoch: 4, loss: 0.72797, train_acc: 0.52\n","Epoch: 5, loss: 0.71229, train_acc: 0.55\n","Epoch: 6, loss: 0.71668, train_acc: 0.52\n","Epoch: 7, loss: 0.72214, train_acc: 0.50\n","Epoch: 8, loss: 0.69553, train_acc: 0.55\n","Epoch: 9, loss: 0.70340, train_acc: 0.52\n","Epoch: 10, loss: 0.69483, train_acc: 0.52\n","Epoch: 11, loss: 0.69725, train_acc: 0.50\n","Epoch: 12, loss: 0.70452, train_acc: 0.51\n","Epoch: 13, loss: 0.67795, train_acc: 0.57\n","Epoch: 14, loss: 0.69159, train_acc: 0.54\n","Epoch: 15, loss: 0.69235, train_acc: 0.54\n","Epoch: 16, loss: 0.70475, train_acc: 0.54\n","Epoch: 17, loss: 0.68420, train_acc: 0.53\n","Epoch: 18, loss: 0.68632, train_acc: 0.58\n","Epoch: 19, loss: 0.68535, train_acc: 0.52\n","Epoch: 20, loss: 0.70252, train_acc: 0.55\n","Epoch: 21, loss: 0.69845, train_acc: 0.50\n","Epoch: 22, loss: 0.68421, train_acc: 0.54\n","Epoch: 23, loss: 0.68995, train_acc: 0.52\n","Epoch: 24, loss: 0.68326, train_acc: 0.52\n","Epoch: 25, loss: 0.69887, train_acc: 0.48\n","Epoch: 26, loss: 0.67554, train_acc: 0.59\n","Epoch: 27, loss: 0.66599, train_acc: 0.66\n","Epoch: 28, loss: 0.68065, train_acc: 0.58\n","Epoch: 29, loss: 0.69890, train_acc: 0.58\n","Epoch: 30, loss: 0.68962, train_acc: 0.56\n","Epoch: 31, loss: 0.66402, train_acc: 0.67\n","Epoch: 32, loss: 0.68347, train_acc: 0.59\n","Epoch: 33, loss: 0.69914, train_acc: 0.49\n","Epoch: 34, loss: 0.70510, train_acc: 0.49\n","Epoch: 35, loss: 0.68822, train_acc: 0.56\n","Epoch: 36, loss: 0.68206, train_acc: 0.58\n","Epoch: 37, loss: 0.68669, train_acc: 0.48\n","Epoch: 38, loss: 0.68201, train_acc: 0.58\n","Epoch: 39, loss: 0.68219, train_acc: 0.57\n","Epoch: 40, loss: 0.68243, train_acc: 0.53\n","Epoch: 41, loss: 0.70272, train_acc: 0.48\n","Epoch: 42, loss: 0.67848, train_acc: 0.60\n","Epoch: 43, loss: 0.67094, train_acc: 0.59\n","Epoch: 44, loss: 0.69157, train_acc: 0.53\n","Epoch: 45, loss: 0.69236, train_acc: 0.54\n","Epoch: 46, loss: 0.71995, train_acc: 0.48\n","Epoch: 47, loss: 0.69331, train_acc: 0.49\n","Epoch: 48, loss: 0.69499, train_acc: 0.51\n","Epoch: 49, loss: 0.67831, train_acc: 0.54\n","Epoch: 50, loss: 0.67603, train_acc: 0.55\n","Epoch: 51, loss: 0.68911, train_acc: 0.55\n","Epoch: 52, loss: 0.65549, train_acc: 0.60\n","Epoch: 53, loss: 0.69064, train_acc: 0.51\n","Epoch: 54, loss: 0.67172, train_acc: 0.59\n","Epoch: 55, loss: 0.67044, train_acc: 0.64\n","Epoch: 56, loss: 0.68233, train_acc: 0.55\n","Epoch: 57, loss: 0.65919, train_acc: 0.66\n","Epoch: 58, loss: 0.66991, train_acc: 0.56\n","Epoch: 59, loss: 0.68758, train_acc: 0.58\n","Epoch: 60, loss: 0.68859, train_acc: 0.56\n","Epoch: 61, loss: 0.69323, train_acc: 0.51\n","Epoch: 62, loss: 0.68364, train_acc: 0.59\n","Epoch: 63, loss: 0.67298, train_acc: 0.58\n","Epoch: 64, loss: 0.69954, train_acc: 0.54\n","Epoch: 65, loss: 0.68919, train_acc: 0.53\n","Epoch: 66, loss: 0.68946, train_acc: 0.55\n","Epoch: 67, loss: 0.67369, train_acc: 0.55\n","Epoch: 68, loss: 0.68653, train_acc: 0.49\n","Epoch: 69, loss: 0.66742, train_acc: 0.62\n","Epoch: 70, loss: 0.67187, train_acc: 0.59\n","Epoch: 71, loss: 0.68840, train_acc: 0.55\n","Epoch: 72, loss: 0.66335, train_acc: 0.62\n","Epoch: 73, loss: 0.68747, train_acc: 0.54\n","Epoch: 74, loss: 0.68551, train_acc: 0.57\n","Epoch: 75, loss: 0.68944, train_acc: 0.50\n","Epoch: 76, loss: 0.68897, train_acc: 0.55\n","Epoch: 77, loss: 0.69339, train_acc: 0.48\n","Epoch: 78, loss: 0.66418, train_acc: 0.62\n","Epoch: 79, loss: 0.67293, train_acc: 0.52\n","Epoch: 80, loss: 0.67772, train_acc: 0.55\n","Epoch: 81, loss: 0.66604, train_acc: 0.64\n","Epoch: 82, loss: 0.68517, train_acc: 0.54\n","Epoch: 83, loss: 0.68277, train_acc: 0.56\n","Epoch: 84, loss: 0.67740, train_acc: 0.55\n","Epoch: 85, loss: 0.67900, train_acc: 0.55\n","Epoch: 86, loss: 0.66948, train_acc: 0.55\n","Epoch: 87, loss: 0.65719, train_acc: 0.56\n","Epoch: 88, loss: 0.68683, train_acc: 0.53\n","Epoch: 89, loss: 0.67861, train_acc: 0.54\n","Epoch: 90, loss: 0.68422, train_acc: 0.52\n","Epoch: 91, loss: 0.67893, train_acc: 0.55\n","Epoch: 92, loss: 0.67457, train_acc: 0.60\n","Epoch: 93, loss: 0.68253, train_acc: 0.54\n","Epoch: 94, loss: 0.66675, train_acc: 0.64\n","Epoch: 95, loss: 0.68248, train_acc: 0.58\n","Epoch: 96, loss: 0.68572, train_acc: 0.55\n","Epoch: 97, loss: 0.69124, train_acc: 0.53\n","Epoch: 98, loss: 0.65744, train_acc: 0.69\n","Epoch: 99, loss: 0.66091, train_acc: 0.61\n","Epoch: 100, loss: 0.68087, train_acc: 0.55\n","Epoch: 101, loss: 0.67754, train_acc: 0.60\n","Epoch: 102, loss: 0.67371, train_acc: 0.59\n","Epoch: 103, loss: 0.67428, train_acc: 0.60\n","Epoch: 104, loss: 0.66367, train_acc: 0.58\n","Epoch: 105, loss: 0.69380, train_acc: 0.55\n","Epoch: 106, loss: 0.67970, train_acc: 0.56\n","Epoch: 107, loss: 0.66126, train_acc: 0.59\n","Epoch: 108, loss: 0.67378, train_acc: 0.57\n","Epoch: 109, loss: 0.69296, train_acc: 0.54\n","Epoch: 110, loss: 0.66329, train_acc: 0.63\n","Epoch: 111, loss: 0.65905, train_acc: 0.62\n","Epoch: 112, loss: 0.66663, train_acc: 0.59\n","Epoch: 113, loss: 0.66041, train_acc: 0.66\n","Epoch: 114, loss: 0.70629, train_acc: 0.55\n","Epoch: 115, loss: 0.66858, train_acc: 0.59\n","Epoch: 116, loss: 0.68757, train_acc: 0.55\n","Epoch: 117, loss: 0.65224, train_acc: 0.67\n","Epoch: 118, loss: 0.67931, train_acc: 0.57\n","Epoch: 119, loss: 0.67648, train_acc: 0.53\n","Epoch: 120, loss: 0.69728, train_acc: 0.52\n","Epoch: 121, loss: 0.67324, train_acc: 0.57\n","Epoch: 122, loss: 0.68460, train_acc: 0.56\n","Epoch: 123, loss: 0.66985, train_acc: 0.64\n","Epoch: 124, loss: 0.66575, train_acc: 0.57\n","Epoch: 125, loss: 0.64438, train_acc: 0.62\n","Finished Training\n","              precision    recall  f1-score   support\n","\n","           0     0.5683    0.7666    0.6527       467\n","           1     0.5401    0.3200    0.4019       400\n","\n","    accuracy                         0.5606       867\n","   macro avg     0.5542    0.5433    0.5273       867\n","weighted avg     0.5553    0.5606    0.5370       867\n","\n","F1 score of data processing without URL + without stop words + lower case + stemming + lemmatizes,\n","word embedding: FastText(SG) + glove-twitter-200,\n"," model:Bi-LSTM : 53.698%\n"]}],"source":["\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    # Accuracy of training data\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","    print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, loss.item(), acc))\n","\n","print('Finished Training')\n","\n","# F1 score on test data\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","# classification_report builds a text report showing the main classification metrics\n","print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","perprocess= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')\n","print('F1 score of data processing without URL + without stop words + lower case + stemming + lemmatizes,\\nword embedding: FastText(SG) + glove-twitter-200,\\n model:Bi-LSTM : {:.3f}%'.format(perprocess*100)) #48.717%"]},{"cell_type":"markdown","metadata":{"id":"a4mpRpocePLN"},"source":["# 4 - Evaluation\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lbLBzHObsvvM"},"source":["## 4.1. Word Embedding Evaluation\n"]},{"cell_type":"markdown","metadata":{"id":"kKexO1jwRH2q"},"source":["The code compares the performance of word embedding with dimension of 100, 200, 250 and window sizes of 3, 5, 10.\n","The results show that when the dimension is 100, which is the smallest of the three selected dimensions, the accuracy rate is the highest.\n","Similarly, 3 is the smallest among three selected window sizes and has the highest accuracy.\n","When the window size is small, this word embedding represents a closer relationship with similar words. If the window size is larger, the word embedding will become very general, because the information contained in each embedding has a lot of overlap. Therefore, it is reasonable that when window size equals to 3, the model present in a more accurate way. Since small window size can help word embedding contain more precise and specific meaning.  \n","\n","Dimension controls the word embedding length, the longer size of embedding, the more information it contains. Theoretically, the larger the dimension, the better it performs, but according to the experiment, when the dimension is 100, the best accuracy is obtained in the test. It may be because we don't have enough data to train it, since the high dimension requires more data for calculation. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2614242,"status":"ok","timestamp":1650003819590,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"vvTBZ4c_QsN-","outputId":"1025878e-0bd3-4f1f-e984-5833cc0a4347","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensions: 100, window sizes: 5\n","Cloning into 'GloVe'...\n","remote: Enumerating objects: 606, done.\u001b[K\n","remote: Counting objects: 100% (14/14), done.\u001b[K\n","remote: Compressing objects: 100% (12/12), done.\u001b[K\n","remote: Total 606 (delta 5), reused 7 (delta 2), pack-reused 592\u001b[K\n","Receiving objects: 100% (606/606), 224.91 KiB | 1.99 MiB/s, done.\n","Resolving deltas: 100% (343/343), done.\n","Vocab size:  33573\n","capital-common-countries.txt:\n","ACCURACY TOP1: 1.11% (1/90)\n","capital-world.txt:\n","ACCURACY TOP1: 0.00% (0/88)\n","currency.txt:\n","ACCURACY TOP1: 0.00% (0/18)\n","city-in-state.txt:\n","ACCURACY TOP1: 1.81% (6/331)\n","family.txt:\n","ACCURACY TOP1: 31.67% (76/240)\n","gram1-adjective-to-adverb.txt:\n","ACCURACY TOP1: 51.84% (451/870)\n","gram2-opposite.txt:\n","ACCURACY TOP1: 59.00% (354/600)\n","gram3-comparative.txt:\n","ACCURACY TOP1: 44.67% (595/1332)\n","gram4-superlative.txt:\n","ACCURACY TOP1: 60.86% (566/930)\n","gram5-present-participle.txt:\n","ACCURACY TOP1: 69.57% (647/930)\n","gram6-nationality-adjective.txt:\n","ACCURACY TOP1: 43.16% (366/848)\n","gram7-past-tense.txt:\n","ACCURACY TOP1: 30.70% (455/1482)\n","ERROR: no lines of vocab kept for gram8-plural.txt !\n","Example missing line: ['banana', 'bananas', 'bird', 'birds']\n","gram9-plural-verbs.txt:\n","ACCURACY TOP1: 59.09% (78/132)\n","Questions seen/total: 40.38% (7891/19544)\n","Semantic accuracy: 10.82%  (83/767)\n","Syntactic accuracy: 49.30%  (3512/7124)\n","Total accuracy: 45.56%  (3595/7891)\n","========================================\n","Dimensions: 200, window sizes: 3\n","fatal: destination path 'GloVe' already exists and is not an empty directory.\n","Vocab size:  33573\n","capital-common-countries.txt:\n","ACCURACY TOP1: 3.33% (3/90)\n","capital-world.txt:\n","ACCURACY TOP1: 0.00% (0/88)\n","currency.txt:\n","ACCURACY TOP1: 5.56% (1/18)\n","city-in-state.txt:\n","ACCURACY TOP1: 3.02% (10/331)\n","family.txt:\n","ACCURACY TOP1: 23.75% (57/240)\n","gram1-adjective-to-adverb.txt:\n","ACCURACY TOP1: 55.75% (485/870)\n","gram2-opposite.txt:\n","ACCURACY TOP1: 70.67% (424/600)\n","gram3-comparative.txt:\n","ACCURACY TOP1: 49.32% (657/1332)\n","gram4-superlative.txt:\n","ACCURACY TOP1: 74.41% (692/930)\n","gram5-present-participle.txt:\n","ACCURACY TOP1: 63.87% (594/930)\n","gram6-nationality-adjective.txt:\n","ACCURACY TOP1: 46.70% (396/848)\n","gram7-past-tense.txt:\n","ACCURACY TOP1: 23.75% (352/1482)\n","ERROR: no lines of vocab kept for gram8-plural.txt !\n","Example missing line: ['banana', 'bananas', 'bird', 'birds']\n","gram9-plural-verbs.txt:\n","ACCURACY TOP1: 70.45% (93/132)\n","Questions seen/total: 40.38% (7891/19544)\n","Semantic accuracy: 9.26%  (71/767)\n","Syntactic accuracy: 51.84%  (3693/7124)\n","Total accuracy: 47.70%  (3764/7891)\n","========================================\n","Dimensions: 200, window sizes: 5\n","fatal: destination path 'GloVe' already exists and is not an empty directory.\n","Vocab size:  33573\n","capital-common-countries.txt:\n","ACCURACY TOP1: 3.33% (3/90)\n","capital-world.txt:\n","ACCURACY TOP1: 0.00% (0/88)\n","currency.txt:\n","ACCURACY TOP1: 0.00% (0/18)\n","city-in-state.txt:\n","ACCURACY TOP1: 1.81% (6/331)\n","family.txt:\n","ACCURACY TOP1: 28.75% (69/240)\n","gram1-adjective-to-adverb.txt:\n","ACCURACY TOP1: 49.43% (430/870)\n","gram2-opposite.txt:\n","ACCURACY TOP1: 63.17% (379/600)\n","gram3-comparative.txt:\n","ACCURACY TOP1: 44.97% (599/1332)\n","gram4-superlative.txt:\n","ACCURACY TOP1: 69.14% (643/930)\n","gram5-present-participle.txt:\n","ACCURACY TOP1: 57.63% (536/930)\n","gram6-nationality-adjective.txt:\n","ACCURACY TOP1: 49.88% (423/848)\n","gram7-past-tense.txt:\n","ACCURACY TOP1: 21.05% (312/1482)\n","ERROR: no lines of vocab kept for gram8-plural.txt !\n","Example missing line: ['banana', 'bananas', 'bird', 'birds']\n","gram9-plural-verbs.txt:\n","ACCURACY TOP1: 69.70% (92/132)\n","Questions seen/total: 40.38% (7891/19544)\n","Semantic accuracy: 10.17%  (78/767)\n","Syntactic accuracy: 47.92%  (3414/7124)\n","Total accuracy: 44.25%  (3492/7891)\n","========================================\n","Dimension: 250, window sizes: 5\n","fatal: destination path 'GloVe' already exists and is not an empty directory.\n","Vocab size:  33573\n","capital-common-countries.txt:\n","ACCURACY TOP1: 0.00% (0/90)\n","capital-world.txt:\n","ACCURACY TOP1: 0.00% (0/88)\n","currency.txt:\n","ACCURACY TOP1: 5.56% (1/18)\n","city-in-state.txt:\n","ACCURACY TOP1: 1.81% (6/331)\n","family.txt:\n","ACCURACY TOP1: 25.83% (62/240)\n","gram1-adjective-to-adverb.txt:\n","ACCURACY TOP1: 49.66% (432/870)\n","gram2-opposite.txt:\n","ACCURACY TOP1: 62.67% (376/600)\n","gram3-comparative.txt:\n","ACCURACY TOP1: 41.29% (550/1332)\n","gram4-superlative.txt:\n","ACCURACY TOP1: 68.17% (634/930)\n","gram5-present-participle.txt:\n","ACCURACY TOP1: 51.29% (477/930)\n","gram6-nationality-adjective.txt:\n","ACCURACY TOP1: 47.64% (404/848)\n","gram7-past-tense.txt:\n","ACCURACY TOP1: 19.77% (293/1482)\n","ERROR: no lines of vocab kept for gram8-plural.txt !\n","Example missing line: ['banana', 'bananas', 'bird', 'birds']\n","gram9-plural-verbs.txt:\n","ACCURACY TOP1: 58.33% (77/132)\n","Questions seen/total: 40.38% (7891/19544)\n","Semantic accuracy: 9.00%  (69/767)\n","Syntactic accuracy: 45.52%  (3243/7124)\n","Total accuracy: 41.97%  (3312/7891)\n","========================================\n","Dimensions: 200, window sizes: 10\n","fatal: destination path 'GloVe' already exists and is not an empty directory.\n","Vocab size:  33573\n","capital-common-countries.txt:\n","ACCURACY TOP1: 0.00% (0/90)\n","capital-world.txt:\n","ACCURACY TOP1: 0.00% (0/88)\n","currency.txt:\n","ACCURACY TOP1: 0.00% (0/18)\n","city-in-state.txt:\n","ACCURACY TOP1: 2.72% (9/331)\n","family.txt:\n","ACCURACY TOP1: 31.67% (76/240)\n","gram1-adjective-to-adverb.txt:\n","ACCURACY TOP1: 42.30% (368/870)\n","gram2-opposite.txt:\n","ACCURACY TOP1: 38.50% (231/600)\n","gram3-comparative.txt:\n","ACCURACY TOP1: 32.36% (431/1332)\n","gram4-superlative.txt:\n","ACCURACY TOP1: 52.47% (488/930)\n","gram5-present-participle.txt:\n","ACCURACY TOP1: 56.56% (526/930)\n","gram6-nationality-adjective.txt:\n","ACCURACY TOP1: 50.35% (427/848)\n","gram7-past-tense.txt:\n","ACCURACY TOP1: 20.99% (311/1482)\n","ERROR: no lines of vocab kept for gram8-plural.txt !\n","Example missing line: ['banana', 'bananas', 'bird', 'birds']\n","gram9-plural-verbs.txt:\n","ACCURACY TOP1: 62.88% (83/132)\n","Questions seen/total: 40.38% (7891/19544)\n","Semantic accuracy: 11.08%  (85/767)\n","Syntactic accuracy: 40.22%  (2865/7124)\n","Total accuracy: 37.38%  (2950/7891)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","sem_acc_win={}\n","syn_acc_win ={}\n","tot_acc_win ={}\n","sem_acc_dim={}\n","syn_acc_dim={}\n","tot_acc_dim={}\n","\n","# Word embedding 1\n","# Dimensions: 100, window sizes: 5\n","print('Dimensions: 100, window sizes: 5')\n","from gensim.models import FastText\n","ft_sg_model_1 = FastText(model_train_docs, size=100, window=5, min_count=5, workers=2, sg=1)\n","ft_sg_model_1.wv.save_word2vec_format('ted_ft_w2v_1.txt', binary=False)\n","!git clone https://github.com/stanfordnlp/GloVe.git\n","vectors_file=\"/content/ted_ft_w2v_1.txt\"\n","\n","with open(vectors_file, 'r') as f:\n","  vectors = {}\n","  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n","    vals = line.rstrip().split(' ')\n","    vectors[vals[0]] = [float(x) for x in vals[1:]]\n","\n","vocab_words=list(vectors.keys())\n","vocab_size = len(vocab_words)\n","print(\"Vocab size: \",str(vocab_size))\n","\n","# create word->index and index->word converter\n","vocab = {w: idx for idx, w in enumerate(vocab_words)}\n","ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n","\n","# create the embedding matrix of shape (vocab_size, dim)\n","vector_dim = len(vectors[ivocab[0]])\n","W = np.zeros((vocab_size, vector_dim))\n","for word, v in vectors.items():\n","    if word == '<unk>':\n","        continue\n","    W[vocab[word], :] = v\n","\n","# normalize each word vector to unit length\n","# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n","W_norm = np.zeros(W.shape)\n","d = (np.sum(W ** 2, 1) ** (0.5))\n","W_norm = (W.T / d).T\n","\n","def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n","    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n","\n","    filenames = [\n","        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n","        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n","        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n","        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n","        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n","        ]\n","\n","    # to avoid memory overflow, could be increased/decreased\n","    # depending on system and vocab size\n","    split_size = 100\n","    correct_sem = 0; # count correct semantic questions\n","    correct_syn = 0; # count correct syntactic questions\n","    correct_tot = 0 # count correct questions\n","    count_sem = 0; # count all semantic questions\n","    count_syn = 0; # count all syntactic questions\n","    count_tot = 0 # count all questions\n","    full_count = 0 # count all questions, including those with unknown words\n","\n","    for i in range(len(filenames)):\n","        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n","            full_data = [line.rstrip().split(' ') for line in f]\n","            full_count += len(full_data)\n","            data = [x for x in full_data if all(word in vocab for word in x)]\n","\n","        if len(data) == 0:\n","            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n","            print(\"Example missing line:\", full_data[0])\n","            continue\n","\n","        indices = np.array([[vocab[word] for word in row] for row in data])\n","        ind1, ind2, ind3, ind4 = indices.T\n","\n","        predictions = np.zeros((len(indices),))\n","        num_iter = int(np.ceil(len(indices) / float(split_size)))\n","        for j in range(num_iter):\n","            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n","\n","            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n","                +  W[ind3[subset], :])\n","\n","            #cosine similarity if input W has been normalized\n","            dist = np.dot(W, pred_vec.T)\n","\n","            for k in range(len(subset)):\n","                dist[ind1[subset[k]], k] = -np.Inf\n","                dist[ind2[subset[k]], k] = -np.Inf\n","                dist[ind3[subset[k]], k] = -np.Inf\n","\n","            # predicted word index\n","            predictions[subset] = np.argmax(dist, 0).flatten()\n","   \n","        val = (ind4 == predictions) # correct predictions\n","        count_tot = count_tot + len(ind1)\n","        correct_tot = correct_tot + sum(val)\n","        if i < 5:\n","            count_sem = count_sem + len(ind1)\n","            correct_sem = correct_sem + sum(val)\n","        else:\n","            count_syn = count_syn + len(ind1)\n","            correct_syn = correct_syn + sum(val)\n","\n","        print(\"%s:\" % filenames[i])\n","        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n","            (np.mean(val) * 100, np.sum(val), len(val)))\n","        \n","    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n","\n","correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n","print('Questions seen/total: %.2f%% (%d/%d)' %\n","    (100 * count_tot / float(full_count), count_tot, full_count))\n","print('Semantic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n","print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n","print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n","\n","\n","sem_acc_dim[100] =  (100 * correct_sem / float(count_sem), correct_sem, count_sem)\n","syn_acc_dim[100]= (100 * correct_syn / float(count_syn), correct_syn, count_syn)\n","tot_acc_dim[100] = (100 * correct_tot / float(count_tot), correct_tot, count_tot)\n","print('='*40)\n","\n","print('Dimensions: 200, window sizes: 3')\n","ft_sg_model_3 = FastText(model_train_docs, size=200, window=3, min_count=5, workers=2, sg=1)\n","ft_sg_model_3.wv.save_word2vec_format('ted_ft_w2v_3.txt', binary=False)\n","!git clone https://github.com/stanfordnlp/GloVe.git\n","vectors_file=\"/content/ted_ft_w2v_3.txt\"\n","\n","with open(vectors_file, 'r') as f:\n","  vectors = {}\n","  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n","    vals = line.rstrip().split(' ')\n","    vectors[vals[0]] = [float(x) for x in vals[1:]]\n","\n","vocab_words=list(vectors.keys())\n","vocab_size = len(vocab_words)\n","print(\"Vocab size: \",str(vocab_size))\n","\n","# create word->index and index->word converter\n","vocab = {w: idx for idx, w in enumerate(vocab_words)}\n","ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n","\n","# create the embedding matrix of shape (vocab_size, dim)\n","vector_dim = len(vectors[ivocab[0]])\n","W = np.zeros((vocab_size, vector_dim))\n","for word, v in vectors.items():\n","    if word == '<unk>':\n","        continue\n","    W[vocab[word], :] = v\n","\n","# normalize each word vector to unit length\n","# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n","W_norm = np.zeros(W.shape)\n","d = (np.sum(W ** 2, 1) ** (0.5))\n","W_norm = (W.T / d).T\n","\n","\n","def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n","    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n","\n","    filenames = [\n","        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n","        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n","        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n","        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n","        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n","        ]\n","\n","    # to avoid memory overflow, could be increased/decreased\n","    # depending on system and vocab size\n","    split_size = 100\n","\n","    correct_sem = 0; # count correct semantic questions\n","    correct_syn = 0; # count correct syntactic questions\n","    correct_tot = 0 # count correct questions\n","    count_sem = 0; # count all semantic questions\n","    count_syn = 0; # count all syntactic questions\n","    count_tot = 0 # count all questions\n","    full_count = 0 # count all questions, including those with unknown words\n","\n","    for i in range(len(filenames)):\n","        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n","            full_data = [line.rstrip().split(' ') for line in f]\n","            full_count += len(full_data)\n","            data = [x for x in full_data if all(word in vocab for word in x)]\n","\n","        if len(data) == 0:\n","            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n","            print(\"Example missing line:\", full_data[0])\n","            continue\n","\n","        indices = np.array([[vocab[word] for word in row] for row in data])\n","        ind1, ind2, ind3, ind4 = indices.T\n","\n","        predictions = np.zeros((len(indices),))\n","        num_iter = int(np.ceil(len(indices) / float(split_size)))\n","        for j in range(num_iter):\n","            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n","\n","            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n","                +  W[ind3[subset], :])\n","\n","            #cosine similarity if input W has been normalized\n","            dist = np.dot(W, pred_vec.T)\n","\n","\n","            for k in range(len(subset)):\n","                dist[ind1[subset[k]], k] = -np.Inf\n","                dist[ind2[subset[k]], k] = -np.Inf\n","                dist[ind3[subset[k]], k] = -np.Inf\n","\n","            # predicted word index\n","            predictions[subset] = np.argmax(dist, 0).flatten()\n","\n","        \n","        val = (ind4 == predictions) # correct predictions\n","        count_tot = count_tot + len(ind1)\n","        correct_tot = correct_tot + sum(val)\n","        if i < 5:\n","            count_sem = count_sem + len(ind1)\n","            correct_sem = correct_sem + sum(val)\n","        else:\n","            count_syn = count_syn + len(ind1)\n","            correct_syn = correct_syn + sum(val)\n","\n","        print(\"%s:\" % filenames[i])\n","        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n","            (np.mean(val) * 100, np.sum(val), len(val)))\n","        \n","    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n","\n","correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n","print('Questions seen/total: %.2f%% (%d/%d)' %\n","    (100 * count_tot / float(full_count), count_tot, full_count))\n","print('Semantic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n","print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n","print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n","print('='*40)\n","sem_acc_win[3] =  (100 * correct_sem / float(count_sem), correct_sem, count_sem)\n","syn_acc_win[3]= (100 * correct_syn / float(count_syn), correct_syn, count_syn)\n","tot_acc_win[3] = (100 * correct_tot / float(count_tot), correct_tot, count_tot)\n","\n","\n","# Word embedding 2\n","# Dimensions: 200, window sizes: 5\n","print('Dimensions: 200, window sizes: 5')\n","ft_sg_model_3 = FastText(model_train_docs, size=200, window=5, min_count=5, workers=2, sg=1)\n","ft_sg_model_3.wv.save_word2vec_format('ted_ft_w2v_3.txt', binary=False)\n","!git clone https://github.com/stanfordnlp/GloVe.git\n","vectors_file=\"/content/ted_ft_w2v_3.txt\"\n","\n","with open(vectors_file, 'r') as f:\n","  vectors = {}\n","  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n","    vals = line.rstrip().split(' ')\n","    vectors[vals[0]] = [float(x) for x in vals[1:]]\n","\n","vocab_words=list(vectors.keys())\n","vocab_size = len(vocab_words)\n","print(\"Vocab size: \",str(vocab_size))\n","\n","# create word->index and index->word converter\n","vocab = {w: idx for idx, w in enumerate(vocab_words)}\n","ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n","\n","# create the embedding matrix of shape (vocab_size, dim)\n","vector_dim = len(vectors[ivocab[0]])\n","W = np.zeros((vocab_size, vector_dim))\n","for word, v in vectors.items():\n","    if word == '<unk>':\n","        continue\n","    W[vocab[word], :] = v\n","\n","# normalize each word vector to unit length\n","# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n","W_norm = np.zeros(W.shape)\n","d = (np.sum(W ** 2, 1) ** (0.5))\n","W_norm = (W.T / d).T\n","\n","\n","def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n","    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n","\n","    filenames = [\n","        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n","        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n","        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n","        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n","        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n","        ]\n","\n","    # to avoid memory overflow, could be increased/decreased\n","    # depending on system and vocab size\n","    split_size = 100\n","\n","    correct_sem = 0; # count correct semantic questions\n","    correct_syn = 0; # count correct syntactic questions\n","    correct_tot = 0 # count correct questions\n","    count_sem = 0; # count all semantic questions\n","    count_syn = 0; # count all syntactic questions\n","    count_tot = 0 # count all questions\n","    full_count = 0 # count all questions, including those with unknown words\n","\n","    for i in range(len(filenames)):\n","        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n","            full_data = [line.rstrip().split(' ') for line in f]\n","            full_count += len(full_data)\n","            data = [x for x in full_data if all(word in vocab for word in x)]\n","\n","        if len(data) == 0:\n","            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n","            print(\"Example missing line:\", full_data[0])\n","            continue\n","\n","        indices = np.array([[vocab[word] for word in row] for row in data])\n","        ind1, ind2, ind3, ind4 = indices.T\n","\n","        predictions = np.zeros((len(indices),))\n","        num_iter = int(np.ceil(len(indices) / float(split_size)))\n","        for j in range(num_iter):\n","            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n","\n","            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n","                +  W[ind3[subset], :])\n","\n","            #cosine similarity if input W has been normalized\n","            dist = np.dot(W, pred_vec.T)\n","\n","\n","            for k in range(len(subset)):\n","                dist[ind1[subset[k]], k] = -np.Inf\n","                dist[ind2[subset[k]], k] = -np.Inf\n","                dist[ind3[subset[k]], k] = -np.Inf\n","\n","            # predicted word index\n","            predictions[subset] = np.argmax(dist, 0).flatten()\n","\n","        \n","        val = (ind4 == predictions) # correct predictions\n","        count_tot = count_tot + len(ind1)\n","        correct_tot = correct_tot + sum(val)\n","        if i < 5:\n","            count_sem = count_sem + len(ind1)\n","            correct_sem = correct_sem + sum(val)\n","        else:\n","            count_syn = count_syn + len(ind1)\n","            correct_syn = correct_syn + sum(val)\n","\n","        print(\"%s:\" % filenames[i])\n","        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n","            (np.mean(val) * 100, np.sum(val), len(val)))\n","        \n","    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n","\n","correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n","print('Questions seen/total: %.2f%% (%d/%d)' %\n","    (100 * count_tot / float(full_count), count_tot, full_count))\n","print('Semantic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n","print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n","print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n","\n","sem_acc_dim[200] =  (100 * correct_sem / float(count_sem), correct_sem, count_sem)\n","syn_acc_dim[200]= (100 * correct_syn / float(count_syn), correct_syn, count_syn)\n","tot_acc_dim[200] = (100 * correct_tot / float(count_tot), correct_tot, count_tot)\n","sem_acc_win[5] =  (100 * correct_sem / float(count_sem), correct_sem, count_sem)\n","syn_acc_win[5]= (100 * correct_syn / float(count_syn), correct_syn, count_syn)\n","tot_acc_win[5] = (100 * correct_tot / float(count_tot), correct_tot, count_tot)\n","print('='*40)\n","\n","\n","# Word embedding 3\n","# Dimensions: 250, window sizes: 5\n","print('Dimension: 250, window sizes: 5')\n","ft_sg_model_4 = FastText(model_train_docs, size=250, window=5, min_count=5, workers=2, sg=1)\n","ft_sg_model_4.wv.save_word2vec_format('ted_ft_w2v_4.txt', binary=False)\n","!git clone https://github.com/stanfordnlp/GloVe.git\n","vectors_file=\"/content/ted_ft_w2v_4.txt\"\n","\n","with open(vectors_file, 'r') as f:\n","  vectors = {}\n","  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n","    vals = line.rstrip().split(' ')\n","    vectors[vals[0]] = [float(x) for x in vals[1:]]\n","\n","vocab_words=list(vectors.keys())\n","vocab_size = len(vocab_words)\n","print(\"Vocab size: \",str(vocab_size))\n","\n","# create word->index and index->word converter\n","vocab = {w: idx for idx, w in enumerate(vocab_words)}\n","ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n","\n","# create the embedding matrix of shape (vocab_size, dim)\n","vector_dim = len(vectors[ivocab[0]])\n","W = np.zeros((vocab_size, vector_dim))\n","for word, v in vectors.items():\n","    if word == '<unk>':\n","        continue\n","    W[vocab[word], :] = v\n","\n","# normalize each word vector to unit length\n","# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n","W_norm = np.zeros(W.shape)\n","d = (np.sum(W ** 2, 1) ** (0.5))\n","W_norm = (W.T / d).T\n","\n","\n","def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n","    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n","\n","    filenames = [\n","        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n","        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n","        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n","        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n","        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n","        ]\n","\n","    # to avoid memory overflow, could be increased/decreased\n","    # depending on system and vocab size\n","    split_size = 100\n","\n","    correct_sem = 0; # count correct semantic questions\n","    correct_syn = 0; # count correct syntactic questions\n","    correct_tot = 0 # count correct questions\n","    count_sem = 0; # count all semantic questions\n","    count_syn = 0; # count all syntactic questions\n","    count_tot = 0 # count all questions\n","    full_count = 0 # count all questions, including those with unknown words\n","\n","    for i in range(len(filenames)):\n","        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n","            full_data = [line.rstrip().split(' ') for line in f]\n","            full_count += len(full_data)\n","            data = [x for x in full_data if all(word in vocab for word in x)]\n","\n","        if len(data) == 0:\n","            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n","            print(\"Example missing line:\", full_data[0])\n","            continue\n","\n","        indices = np.array([[vocab[word] for word in row] for row in data])\n","        ind1, ind2, ind3, ind4 = indices.T\n","\n","        predictions = np.zeros((len(indices),))\n","        num_iter = int(np.ceil(len(indices) / float(split_size)))\n","        for j in range(num_iter):\n","            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n","\n","            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n","                +  W[ind3[subset], :])\n","\n","            #cosine similarity if input W has been normalized\n","            dist = np.dot(W, pred_vec.T)\n","\n","\n","            for k in range(len(subset)):\n","                dist[ind1[subset[k]], k] = -np.Inf\n","                dist[ind2[subset[k]], k] = -np.Inf\n","                dist[ind3[subset[k]], k] = -np.Inf\n","\n","            # predicted word index\n","            predictions[subset] = np.argmax(dist, 0).flatten()\n","\n","        \n","        val = (ind4 == predictions) # correct predictions\n","        count_tot = count_tot + len(ind1)\n","        correct_tot = correct_tot + sum(val)\n","        if i < 5:\n","            count_sem = count_sem + len(ind1)\n","            correct_sem = correct_sem + sum(val)\n","        else:\n","            count_syn = count_syn + len(ind1)\n","            correct_syn = correct_syn + sum(val)\n","\n","        print(\"%s:\" % filenames[i])\n","        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n","            (np.mean(val) * 100, np.sum(val), len(val)))\n","        \n","    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n","\n","correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n","print('Questions seen/total: %.2f%% (%d/%d)' %\n","    (100 * count_tot / float(full_count), count_tot, full_count))\n","print('Semantic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n","print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n","print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n","\n","sem_acc_dim[250] =  (100 * correct_sem / float(count_sem), correct_sem, count_sem)\n","syn_acc_dim[250]= (100 * correct_syn / float(count_syn), correct_syn, count_syn)\n","tot_acc_dim[250] = (100 * correct_tot / float(count_tot), correct_tot, count_tot)\n","print('='*40)\n","\n","\n","# Word embedding 4\n","# Dimensions: 200, window sizes: 10\n","print('Dimensions: 200, window sizes: 10')\n","ft_sg_model_2 = FastText(model_train_docs, size=200, window=10, min_count=5, workers=2, sg=1)\n","ft_sg_model_2.wv.save_word2vec_format('ted_ft_w2v_2.txt', binary=False)\n","!git clone https://github.com/stanfordnlp/GloVe.git\n","vectors_file=\"/content/ted_ft_w2v_2.txt\"\n","\n","with open(vectors_file, 'r') as f:\n","  vectors = {}\n","  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n","    vals = line.rstrip().split(' ')\n","    vectors[vals[0]] = [float(x) for x in vals[1:]]\n","\n","vocab_words=list(vectors.keys())\n","vocab_size = len(vocab_words)\n","print(\"Vocab size: \",str(vocab_size))\n","\n","# create word->index and index->word converter\n","vocab = {w: idx for idx, w in enumerate(vocab_words)}\n","ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n","\n","# create the embedding matrix of shape (vocab_size, dim)\n","vector_dim = len(vectors[ivocab[0]])\n","W = np.zeros((vocab_size, vector_dim))\n","for word, v in vectors.items():\n","    if word == '<unk>':\n","        continue\n","    W[vocab[word], :] = v\n","\n","# normalize each word vector to unit length\n","# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n","W_norm = np.zeros(W.shape)\n","d = (np.sum(W ** 2, 1) ** (0.5))\n","W_norm = (W.T / d).T\n","\n","\n","def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n","    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n","\n","    filenames = [\n","        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n","        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n","        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n","        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n","        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n","        ]\n","\n","    # to avoid memory overflow, could be increased/decreased\n","    # depending on system and vocab size\n","    split_size = 100\n","\n","    correct_sem = 0; # count correct semantic questions\n","    correct_syn = 0; # count correct syntactic questions\n","    correct_tot = 0 # count correct questions\n","    count_sem = 0; # count all semantic questions\n","    count_syn = 0; # count all syntactic questions\n","    count_tot = 0 # count all questions\n","    full_count = 0 # count all questions, including those with unknown words\n","\n","    for i in range(len(filenames)):\n","        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n","            full_data = [line.rstrip().split(' ') for line in f]\n","            full_count += len(full_data)\n","            data = [x for x in full_data if all(word in vocab for word in x)]\n","\n","        if len(data) == 0:\n","            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n","            print(\"Example missing line:\", full_data[0])\n","            continue\n","\n","        indices = np.array([[vocab[word] for word in row] for row in data])\n","        ind1, ind2, ind3, ind4 = indices.T\n","\n","        predictions = np.zeros((len(indices),))\n","        num_iter = int(np.ceil(len(indices) / float(split_size)))\n","        for j in range(num_iter):\n","            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n","\n","            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n","                +  W[ind3[subset], :])\n","\n","            #cosine similarity if input W has been normalized\n","            dist = np.dot(W, pred_vec.T)\n","\n","\n","            for k in range(len(subset)):\n","                dist[ind1[subset[k]], k] = -np.Inf\n","                dist[ind2[subset[k]], k] = -np.Inf\n","                dist[ind3[subset[k]], k] = -np.Inf\n","\n","            # predicted word index\n","            predictions[subset] = np.argmax(dist, 0).flatten()\n","\n","        \n","        val = (ind4 == predictions) # correct predictions\n","        count_tot = count_tot + len(ind1)\n","        correct_tot = correct_tot + sum(val)\n","        if i < 5:\n","            count_sem = count_sem + len(ind1)\n","            correct_sem = correct_sem + sum(val)\n","        else:\n","            count_syn = count_syn + len(ind1)\n","            correct_syn = correct_syn + sum(val)\n","\n","        print(\"%s:\" % filenames[i])\n","        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n","            (np.mean(val) * 100, np.sum(val), len(val)))\n","        \n","    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n","\n","correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n","print('Questions seen/total: %.2f%% (%d/%d)' %\n","    (100 * count_tot / float(full_count), count_tot, full_count))\n","print('Semantic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n","print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n","    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n","print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n","\n","sem_acc_win[10] =  (100 * correct_sem / float(count_sem), correct_sem, count_sem)\n","syn_acc_win[10]= (100 * correct_syn / float(count_syn), correct_syn, count_syn)\n","tot_acc_win[10] = (100 * correct_tot / float(count_tot), correct_tot, count_tot)\n","\n","\n","# plot\n","import matplotlib.pyplot as plt\n","sem_win_x=[]\n","sem_win_y=[]\n","syn_win_x=[]\n","syn_win_y=[]\n","tot_win_x=[]\n","tot_win_y=[]\n","for value in sem_acc_win.values():\n","  sem_win_y.append(value[0])\n","for key in sem_acc_win.keys():\n","  sem_win_x.append(key)\n","for value in syn_acc_win.values():\n","  syn_win_y.append(value[0])\n","for key in syn_acc_win.keys():\n","  syn_win_x.append(key)\n","for value in tot_acc_win.values():\n","  tot_win_y.append(value[0])\n","for key in tot_acc_win.keys():\n","  tot_win_x.append(key)\n","\n","plt.plot(sem_win_x,sem_win_y,label='Semantic')\n","plt.plot(syn_win_x,syn_win_y,label='Syntactic')\n","plt.plot(tot_win_x,tot_win_y,label='Overall')\n","plt.title('Different window sizes accuracy')\n","plt.xlabel('Window sizes')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","import matplotlib.pyplot as plt\n","sem_dim_x=[]\n","sem_dim_y=[]\n","syn_dim_x=[]\n","syn_dim_y=[]\n","tot_dim_x=[]\n","tot_dim_y=[]\n","for value in sem_acc_dim.values():\n","  sem_dim_y.append(value[0])\n","for key in sem_acc_dim.keys():\n","  sem_dim_x.append(key)\n","for value in syn_acc_dim.values():\n","  syn_dim_y.append(value[0])\n","for key in syn_acc_dim.keys():\n","  syn_dim_x.append(key)\n","for value in tot_acc_dim.values():\n","  tot_dim_y.append(value[0])\n","for key in tot_acc_dim.keys():\n","  tot_dim_x.append(key)\n","\n","plt.plot(sem_dim_x,sem_dim_y,label='Semantic')\n","plt.plot(syn_dim_x,syn_dim_y,label='Syntactic')\n","plt.plot(tot_dim_x,tot_dim_y,label='Overall')\n","plt.title('Different Dimension accuracy')\n","plt.xlabel('Dimension sizes')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KEW1zMgVMREr"},"source":["## 4.2. Performance Evaluation with Data Processing Techiques\n"]},{"cell_type":"markdown","metadata":{"id":"TC_5v-GQV3p8"},"source":["The basic operation of the data is to convert it to lowercase, and lemmatizing. On the basis of these two operations, the URL, stop words, and root of the data are processed step by step. By comparison, it can be found the model F1 score trained by data with only the two basic processes is the lowest. Because the URL contains a large number of special symbols or garbled codes, so many addresses have no meaning, which can be seen as useless information. Therefore, URL will only increase the amount of computation, and may mislead the calculation results, which is not helpful for training the model. After deleting the URL, F1 score has increased by nearly 8%. Compared with deleting stop words, keeping stop words seems to be more helpful to the model. It may because some stop words have a certain meaning in this kind of platform, so the stop words are more or less accompanied by some information, which helps to improve the accuracy in analysizing data. When deleting URL, stop words, removing and replacing word suffix, the model performs best. This series of steps can greatly simplify the vocabulary, reduce the computation cost, and reduce the influence of roots or derivations. By filtering these influences, the model can make a more accurate judgment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1974,"status":"ok","timestamp":1650008897189,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"vBrch6wr9QwC","outputId":"304347c7-57d0-43f0-bbf1-22322a20aec3","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["                                      Model     F1 (%)\n","0           Bi-LSTM with URL with stopwords  40.140672\n","1        Bi-LSTM without URL with stopwords  48.332939\n","2     Bi-LSTM without URL without stopwords  46.328540\n","3  Bi-LSTM without (URL + stopwords + stem)  54.618525\n"]}],"source":["# Processing techique 1\n","# with url + lower + lemmatizes\n","# train data set preprocession\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # lower\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  # lemmatizing\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","unique_labels = np.unique(training_labels)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)\n","\n","# word vector\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_dim = ft_sg_model.vector_size #200\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_table.append(ft_sg_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table) \n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*ft_sg_model.vector_size),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        # [IMPORTANT] Initialize the Embedding layer with the lookup table we created \n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        # Optional: set requires_grad = False to make this lookup table untrainable\n","        self.emb.weight.requires_grad = False\n","\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()   \n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step() \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction（testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","print('Bi-LSTM with URL with stopwords:')\n","print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","process_1= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')#44.892%\n","print('='*70)\n","\n","\n","# Process thchnique 2\n","# Withoout url + lower + lemmatizes\n","# Train data set preprocession\n","train_tokensized_docs = []\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  # remove url\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # lower\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  # lemmatizing\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in lower_tokens]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_dim = ft_sg_model.vector_size #200\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_table.append(ft_sg_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table) \n","\n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*ft_sg_model.vector_size),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction（testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","print('Bi-LSTM without URL with stopwords')\n","print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","process_2= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')#49.086%\n","print('='*70)\n","\n","\n","# Process technique 3\n","# Withoout url + lower + without stop words + lemmatizes\n","# Train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  # remove url\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # lower\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  # remove stopwords\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  # lemmatizing\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in tokenized_doc]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in tokenized_doc]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_dim = ft_sg_model.vector_size \n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_table.append(ft_sg_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table) \n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*ft_sg_model.vector_size),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction(testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","print('Bi-LSTM without URL without stopwords')\n","print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","process_3= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')#58.137%\n","print('='*70)\n","\n","\n","# Process technique 4\n","# Withoout url + lower + without stop words + lemmatizes + stem\n","# Train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  # remove url\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # lower\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  # remove stopwords\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  # stemming\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  # lemmatizing\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","# Cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_dim = ft_sg_model.vector_size #100\n","\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_table.append(ft_sg_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","\n","emb_table = np.array(emb_table) \n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*ft_sg_model.vector_size),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","\n","# Prediction(testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) #sent_encode: [[4, 6, 9, 0, 0], [4, 7, 5, 0, 0], [4, 3, 9, 0, 0], [4, 2, 8, 6, 5]]\n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) #label_encoded: [1 1 0 0]\n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","print('Bi-LSTM without (URL + stopwords + stem):')\n","print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","process_4= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')#55.191%\n","print('='*70)\n","\n","process ={'Model': ['Bi-LSTM with URL with stopwords','Bi-LSTM without URL with stopwords', 'Bi-LSTM without URL without stopwords','Bi-LSTM without (URL + stopwords + stem)'],\n","          'F1 (%)': [process_1*100,process_2*100,process_3*100,process_4*100]}\n","\n","print(pd.DataFrame(process))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-gwVpllNoOiY"},"source":["## 4.3. Performance Evaluation with Different Input"]},{"cell_type":"markdown","metadata":{"id":"KltldtTTxlje"},"source":["The performance of the FastText(SG) is better than FastText(CBOW), and Glove-twitter-200 is better than Glove-Twitter-50. CBOW use the surrounding words to predict the central words. However, Skip Gram is to predict the surrounding words with the central words. In Skip Gram, the word vector of the central word will be constantly adjusted by using the prediction results of the surrounding words. Skip Gram makes predictions more often than CBOW. Because each word is used as the central word, the surrounding words are used to make predictions once. Therefore, it is very reasonable that the accuracy of Skip Gram is higher than that of CBOW.     \n","\n","Glove-twitter-200 sets up a vector with a length of 200 to represent words, while glove-twitter-50 sets up a vector with a length of 50 to represent words. The longer the vector, the more information it contains. Therefore, the test results is in line with the concept that glove-twitter-200 is superior to glove-twitter-50.\n","\n","When the two methods of word embedding are combined, FastText(SG) plus glove-twitter-200 performs best, because the calculation results of the two will be integrated in the model, so that better accuracy can be obtained.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3691917,"status":"ok","timestamp":1650031346163,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"r8vHpzHSTOvY","outputId":"12e84827-831f-418a-a186-0992981977ba","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["                                            Model     F1 (%)\n","0                                    FastText(SG)  43.424790\n","1                                  FastText(CBOW)  41.042721\n","2                                glove-twitter-50  37.918086\n","3                               glove-twitter-200  47.585961\n","4  Bi-LSTM with FastText(CBOW) +glove-twitter-200  40.437006\n","5    Bi-LSTM with FastText(SG) +glove-twitter-200  52.760008\n"]}],"source":["# Word vectors evaluation\n","# FastText(SG)\n","# Train data set preprocession\n","wordvec = {}\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  singles = [stemmer.stem(plural) for plural in lower_tokens]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","  \n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  singles = [stemmer.stem(plural) for plural in lower_tokens]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","unique_labels = np.unique(training_labels)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)\n","\n","# word vector\n","# FastText(SG)\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_dim = ft_sg_model.vector_size #200\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_table.append(ft_sg_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction (testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","wordvec_1= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted') #51.577%\n","#print('='*70)\n","\n","\n","# FastText(CBOW)\n","# train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  singles = [stemmer.stem(plural) for plural in lower_tokens]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  singles = [stemmer.stem(plural) for plural in lower_tokens]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector: FastText(CBOW)\n","ft_cbow_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=0)\n","emb_dim = ft_cbow_model.vector_size #200\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_cbow_model:\n","        emb_table.append(ft_cbow_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = ft_cbow_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction(testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","\n","from sklearn.metrics import f1_score\n","wordvec_2= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')\n","#print('='*70)\n","#wordvec={'Model': ['FastText(SG)','FastText(CBOW)'],'F1 (%)':[wordvec_1*100,wordvec_2*100]}\n","\n","\n","\n","# pretrained model: glove-twitter-50\n","#train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","unique_labels = np.unique(training_labels)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)\n","\n","\n","# Pretrained model: glove-twitter-50\n","word_emb_model = api.load(\"glove-twitter-50\") \n","emb_pretrain_dim = word_emb_model.vector_size \n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","emb_dim =  word_emb_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if  word in word_emb_model:\n","        emb_table.append(word_emb_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","\n","# Build model\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()    \n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()   \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction （testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","pertrianed_model_1= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted') #44.038%\n","#print('='*70)\n","\n","\n","# pretrained model: glove-twitter-200\n","#train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","\n","# Pre trained model: glove-twitter-200\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","emb_dim =  word_emb_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if  word in word_emb_model:\n","        emb_table.append(word_emb_model[word])\n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","\n","# Build model\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    \n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction(testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) #sent_encode: [[4, 6, 9, 0, 0], [4, 7, 5, 0, 0], [4, 3, 9, 0, 0], [4, 2, 8, 6, 5]]\n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","pertrianed_model_2= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted') #44.038%\n","\n","\n","\n","# Concatenate\n","# FastText(CBOW) + glove-twitter-200\n","#train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=0)\n","emb_wordvec_dim = ft_sg_model.vector_size \n","emb_wordvec_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_wordvec_table.append(ft_sg_model[word])\n","    else:\n","        emb_wordvec_table.append([0]*emb_wordvec_dim)\n","emb_wordvec_table = np.array(emb_wordvec_table)\n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*emb_wordvec_dim),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","\n","        \n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","# Build model\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()  \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction (testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","concatenation_1= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')#38.887%\n","\n","\n","# Concatenation 2\n","# FastText SG + glove-twitter-200\n","# train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_wordvec_dim = ft_sg_model.vector_size #200\n","emb_wordvec_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_wordvec_table.append(ft_sg_model[word])\n","    else:\n","        emb_wordvec_table.append([0]*emb_wordvec_dim)\n","emb_wordvec_table = np.array(emb_wordvec_table)\n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*emb_wordvec_dim),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","\n","# Build model\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()  \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction (testset)\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","concatenation_2= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')#38.887%\n","\n","word_embed_model={'Model':['FastText(SG)','FastText(CBOW)','glove-twitter-50','glove-twitter-200','Bi-LSTM with FastText(CBOW) +glove-twitter-200','Bi-LSTM with FastText(SG) +glove-twitter-200'],\n","               'F1 (%)':[wordvec_1*100,wordvec_2*100,pertrianed_model_1*100,pertrianed_model_2*100,concatenation_1*100,concatenation_2*100]}\n","\n","print(pd.DataFrame(word_embed_model))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vg08uf3hpyoF"},"source":["## 4.4. Performance Evaluation with Different Sequence Models\n"]},{"cell_type":"markdown","metadata":{"id":"8Z7bCX5qDwqH"},"source":["According to the model evaluation based on testing dataset, LSTM is better than RNN. In the process of RNN calculation, as the time goes on, the model may encounter problems of vanishing gradient or exploding gradient. Compared with RNN, LSTM adds gates to control the data. Gate decides whether to keep or forget the data, thus controlling the data, so LSTM can handle long-term memory better than RNN."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3615344,"status":"ok","timestamp":1650168567549,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"vNpaEGuVYviD","outputId":"f325f84c-ea1b-4c8b-91ee-b3dd4300915b","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 758.5/758.5MB downloaded\n","     Model     F1 (%)\n","0  Bi-LSTM  42.156025\n","1   Bi-RNN  39.478615\n"]}],"source":["# Model 1\n","# Fast Text(CBOW) + glove-twitter-200 + Bi-LSTM\n","# Train data set preprocession\n","\n","model = {}\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","# FastText(CBOW)\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_wordvec_dim = ft_sg_model.vector_size #200\n","emb_wordvec_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_wordvec_table.append(ft_sg_model[word])\n","    else:\n","        emb_wordvec_table.append([0]*emb_wordvec_dim)\n","emb_wordvec_table = np.array(emb_wordvec_table)\n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*emb_wordvec_dim),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","unique_labels = np.unique(training_labels)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)\n","\n","# Build model\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()  \n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()  \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","\n","# Prediction\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print('classification report of Fast Text(SG) + glove-twitter-200 + Bi-LSTM:')\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","model_1= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted')\n","\n","\n","# Model 2\n","# Fast Text(CBOW) + glove-twitter-200 + Bi-RNN\n","# Train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","# FastText(CBOW)\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_wordvec_dim = ft_sg_model.vector_size \n","emb_wordvec_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_wordvec_table.append(ft_sg_model[word])\n","    else:\n","        emb_wordvec_table.append([0]*emb_wordvec_dim)\n","emb_wordvec_table = np.array(emb_wordvec_table)\n","\n","# Pre trained model\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*emb_wordvec_dim),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","unique_labels = np.unique(training_labels)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)\n","\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 100\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_RNN_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_RNN_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.rnn = nn.RNN(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        rnn_out, h_n= self.rnn(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[-2,:,:],h_n[-1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_RNN_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()    \n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device)\n","test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","\n","# Prediction\n","model.eval()\n","outputs = model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","\n","# classification_report builds a text report showing the main classification metrics\n","from sklearn.metrics import classification_report\n","#print('classification report of Fast Text(SG) + glove-twitter-200 + Bi-LSTM:')\n","#print(classification_report(test_targe_torch, predicted.cpu().numpy(),digits=4))\n","from sklearn.metrics import f1_score\n","model_2= f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted') #39.789%\n","model ={'Model':['Bi-LSTM','Bi-RNN'],'F1 (%)':[model_1 *100, model_2*100]}\n","print(pd.DataFrame(model))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"P28Z1k36MZuo"},"source":["## 4.5. HyperParameter Testing\n"]},{"cell_type":"markdown","metadata":{"id":"a_KQqjbmpRfO"},"source":["According to the image, it can be concluded that when the learning rate is 0.01, the model performance is the best. The model with a learning rate of 0.001 has a small learning step, and in each epoch the gradient update slowly. Although the update is stable, it rises very slowly in the early stage, and the F1 score only show a relatively significant increase after 175 epoches. Therefore it needs a large computation cost, if you want to fully train the model with learning rate equals to 0.001. For the model with the learning rate of 0.1, F1 score shows a big fluctuation, which may be because the best point is missed every time during the gradient descent, resulting in the unstable performance of the model. When the learning rate is 0.01, the model performs best. Although the model fluctuates around the 75th epoch, it is still the most stable and best overall compared with the other two. According to the trend of F1 score, we can choose epoch to be 125. During the later stage, the model does not show obviously improvement, so we don't need to spend more computation for slight improvement."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1650084661930,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"bOeZ8RtHxjyK","outputId":"5e6a7e06-b49b-4579-f141-f2c2e72f2fcc","vscode":{"languageId":"python"}},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Please comment your code\n","#train data set preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","\n","#test data preprocession\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","for sentense in testing_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  test_tokensized_docs.append(lemmatizes)\n","\n","model_train_docs=[]\n","for sentence in test_tokensized_docs:\n","  model_train_docs.append(sentence)\n","for sentence in train_tokensized_docs:\n","  model_train_docs.append(sentence)\n","\n","#cread word list \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","\n","# word vector\n","# FastText(SG)\n","ft_sg_model = FastText(model_train_docs, size=100, window=3, min_count=5, workers=2, sg=1)\n","emb_wordvec_dim = ft_sg_model.vector_size #200\n","emb_wordvec_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model:\n","        emb_wordvec_table.append(ft_sg_model[word])\n","    else:\n","        emb_wordvec_table.append([0]*emb_wordvec_dim)\n","emb_wordvec_table = np.array(emb_wordvec_table)\n","\n","# Pre trained model\n","# glove-twitter-200\n","word_emb_model = api.load(\"glove-twitter-200\") \n","emb_pretrain_dim = word_emb_model.vector_size #200\n","emb_pretrain_table = []\n","for i, word in enumerate(word_list):\n","    if word in word_emb_model:\n","        emb_pretrain_table.append(word_emb_model[word])\n","    else:\n","        emb_pretrain_table.append([0]*emb_pretrain_dim)\n","emb_pretrain_table = np.array(emb_pretrain_table)\n","\n","#Input concatenate\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","emb_table = []\n","for i, word in enumerate(word_list):\n","    if word in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],ft_sg_model[word]),0))\n","    elif word in ft_sg_model and word not in word_emb_model:\n","        emb_table.append(np.concatenate(([0]*emb_pretrain_dim,ft_sg_model[word]),0))\n","    elif word not in ft_sg_model and word in word_emb_model:\n","        emb_table.append(np.concatenate((word_emb_model[word],[0]*emb_wordvec_dim),0))    \n","    else:\n","        emb_table.append([0]*emb_dim)\n","emb_table = np.array(emb_table)\n","\n","\n","# Padding and encoding\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","sent_encoded = encode_and_add_padding(train_tokensized_docs, seq_length, word_index)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(training_labels)\n","label_encoded= lEnc.transform(training_labels)\n","unique_labels = np.unique(training_labels)\n","\n","from sklearn.preprocessing import LabelEncoder\n","lEnc = LabelEncoder()\n","lEnc.fit(testing_labels)\n","test_label_encoded= lEnc.transform(testing_labels)\n","\n","# Build model 1\n","# lr = 0.1, epoch = 400\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 200\n","learning_rate = 0.1\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        # Optional: set requires_grad = False to make this lookup table untrainable\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","trian_acc_lr1 =[]\n","train_f1_lr1 = []\n","test_acc_lr1 = []\n","test_f1_lr1 = []\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","    trian_acc_lr1.append(acc)\n","    train_f1_lr1.append(f1_score(targets, predicted.cpu().numpy(),average='weighted'))\n","\n","     ## Prediction(testset)\n","    test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","    test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","    test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device)     \n","    model.eval()\n","    outputs = model(test_input_torch)\n","    predicted = torch.argmax(outputs, 1)\n","    acc= accuracy_score(predicted.cpu().numpy(),test_targe_torch)\n","    test_acc_lr1.append(acc)\n","    test_f1_lr1.append(f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted'))\n","\n","\n","# Build model 2\n","# lr = 0.01, epoch = 400\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 200\n","learning_rate = 0.01\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        # Optional: set requires_grad = False to make this lookup table untrainable\n","        self.emb.weight.requires_grad = False\n","\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","trian_acc_lr2 =[]\n","train_f1_lr2 = []\n","test_acc_lr2 = []\n","test_f1_lr2 = []\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","    trian_acc_lr2.append(acc)\n","    train_f1_lr2.append(f1_score(targets, predicted.cpu().numpy(),average='weighted'))\n","\n","\n","    test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","    test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","    test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","\n","    ## Prediction (testset)\n","    model.eval()\n","    outputs = model(test_input_torch)\n","    predicted = torch.argmax(outputs, 1)\n","    acc= accuracy_score(predicted.cpu().numpy(),test_targe_torch)\n","    test_acc_lr2.append(acc)\n","    test_f1_lr2.append(f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted'))\n","\n","\n","# Build model 3\n","# lr = 0.001, epoch = 400\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","train_sentences_encoded_pad = torch.from_numpy(np.array(sent_encoded)).to(device)\n","train_tags_y_pad = torch.from_numpy(np.array(label_encoded)).view(-1).to(device)\n","from torch.utils.data import TensorDataset\n","train_data = TensorDataset(train_sentences_encoded_pad,train_tags_y_pad)\n","from torch.utils.data import DataLoader\n","batch_size = 128\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n","# shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n","vocab_size = len(word_list)\n","total_epoch = 200\n","learning_rate = 0.001\n","n_hidden =1\n","n_class = len(unique_labels)\n","emb_dim = word_emb_model.vector_size + ft_sg_model.vector_size\n","\n","# Define the model\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        # Initialize the Embedding layer with the lookup table we created \n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","# Move the model to GPU\n","model = Bi_LSTM_Emb().to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","trian_acc_lr3 =[]\n","train_f1_lr3 = []\n","test_acc_lr3 = []\n","test_f1_lr3 = []\n","for epoch in range(total_epoch):  \n","    \n","    # Set the flag to training\n","    model.train()\n","    for sentence,targets in train_loader:\n","        sentence = sentence.to(device)\n","        targets = targets.to(device)\n","\n","    # forward + backward + optimize\n","    optimizer.zero_grad()\n","    outputs = model(sentence) \n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    predicted = torch.argmax(outputs, -1)\n","    acc= accuracy_score(predicted.cpu().numpy(),targets.cpu().numpy())\n","    trian_acc_lr3.append(acc)\n","    train_f1_lr3.append(f1_score(targets, predicted.cpu().numpy(),average='weighted'))\n","\n","    ## Prediction (testset)\n","    test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","    test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) \n","    test_targe_torch = torch.from_numpy(np.array(test_label_encoded)).view(-1).to(device) \n","    model.eval()\n","    outputs = model(test_input_torch)\n","    predicted = torch.argmax(outputs, 1)\n","    acc= accuracy_score(predicted.cpu().numpy(),test_targe_torch)\n","    test_acc_lr3.append(acc)\n","    test_f1_lr3.append(f1_score(test_targe_torch, predicted.cpu().numpy(),average='weighted'))\n","\n","\n","# plot F1 score\n","# epoch =200\n","plt.plot(range(200),test_f1_lr1,label='lr = 0.1')\n","plt.plot(range(200),test_f1_lr2,label='lr = 0.01')\n","plt.plot(range(200),test_f1_lr3,label='lr = 0.001')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('F1')\n","plt.title('Performace of different learning rate')\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dIfa2nm85H9I"},"source":["# 5 - Test model via Colab Form Fields User Interface"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":969893,"status":"ok","timestamp":1650180692129,"user":{"displayName":"Sihui Feng","userId":"17304709665478153830"},"user_tz":-600},"id":"HO_aV5bz5-ry","outputId":"ff8202c2-9f3f-4370-c76d-2776005dcb6f","vscode":{"languageId":"python"}},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["negative\n"]}],"source":["#@title Personality Type Prediction\n","\n","text = \"I am thinking logically\" #@param {type:\"string\"}\n","\n","\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","id = '18GnqBFrXvXEKfuF1ITlczfa53bDSiXHa'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('best_classification_model.pt')\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import gensim.downloader as api\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords as sw\n","from nltk.stem.porter import *\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import FastText\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=Warning)\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import re\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","\n","class Bi_LSTM_Emb(nn.Module):\n","    def __init__(self):\n","        super(Bi_LSTM_Emb, self).__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n","        self.linear = nn.Linear(n_hidden*2, n_class)\n","\n","    def forward(self, x):\n","        # Get the embeded tensor\n","        x = self.emb(x)        \n","        lstm_out, (h_n, c_n) = self.lstm(x)\n","        # concat the last hidden state from two direction\n","        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n","        z = self.linear(hidden_out)\n","        return z\n","\n","\n","\n","id = '16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('training_data.csv')\n","import pandas as pd\n","training_data = pd.read_csv(\"/content/training_data.csv\")\n","# Get the list of training data (posts)\n","training_posts=training_data['posts'].tolist()\n","# Get the list of corresponding labels for the training data (posts)\n","training_labels=training_data['type'].tolist()\n","# Process training data\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","train_tokensized_docs = []\n","for sentense in training_posts:\n","  sentense_reurl = re.sub(pattern, '', sentense)\n","  clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","  tokenized_sentence = word_tokenize(clean_sentence)\n","  # lower \n","  lower_tokens = [t.lower() for t in tokenized_sentence]\n","  #remove stop words\n","  sww = sw.words()\n","  tokenized_doc = [w for w in lower_tokens if not w in sww]\n","  # Stemming\n","  singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","  # Lemmatisation\n","  lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","  train_tokensized_docs.append(lemmatizes)\n","# word list  \n","word_set = set() \n","for sent in train_tokensized_docs:\n","    for word in sent:\n","        word_set.add(word)\n","word_set.add('[PAD]')\n","word_set.add('[UNKNOWN]')\n","word_list = list(word_set) \n","word_list.sort()\n","word_index = {}\n","ind = 0\n","for word in word_list:\n","    word_index[word] = ind\n","    ind += 1\n","  \n","the_saved_model = torch.load('/content/best_classification_model.pt')\n","the_saved_model.eval()\n","\n","\n","pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+')\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","test_tokensized_docs = []\n","sentense_reurl = re.sub(pattern, '', text)\n","clean_sentence = re.sub(r'[^a-zA-Z\\s]','',sentense_reurl)\n","tokenized_sentence = word_tokenize(clean_sentence)\n","lower_tokens = [t.lower() for t in tokenized_sentence]\n","tokenized_doc = [w for w in lower_tokens if not w in sww]\n","singles = [stemmer.stem(plural) for plural in tokenized_doc]\n","lemmatizes = [lemmatizer.lemmatize(word) for word in singles]\n","test_tokensized_docs.append(lemmatizes)\n","seq_length = 700\n","def encode_and_add_padding(sentences, seq_length, word_index):\n","    sent_encoded = []\n","    for sent in sentences:\n","        temp_encoded = [word_index[word] if word in word_index else word_index['[UNKNOWN]'] for word in sent] # if word in sentence, word_index[word]\n","        if len(temp_encoded) < seq_length:\n","            temp_encoded += [word_index['[PAD]']] * (seq_length - len(temp_encoded))\n","        if len(temp_encoded) > seq_length:\n","            temp_encoded=temp_encoded[:seq_length]\n","        sent_encoded.append(temp_encoded)\n","    return sent_encoded\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_sent_encoded = encode_and_add_padding(test_tokensized_docs, seq_length, word_index)\n","test_input_torch = torch.from_numpy(np.array(test_sent_encoded)).to(device) #sent_encode: [[4, 6, 9, 0, 0], [4, 7, 5, 0, 0], [4, 3, 9, 0, 0], [4, 2, 8, 6, 5]]\n","outputs = the_saved_model(test_input_torch)\n","predicted = torch.argmax(outputs, 1)\n","if predicted == 0:\n","  print('positive')\n","else:\n","  print('negative')"]}],"metadata":{"colab":{"collapsed_sections":["MGHoy6KpQDfZ","-gwVpllNoOiY"],"name":"sfen9234_COMP5046_Ass1_2022.ipynb","provenance":[{"file_id":"16YC0W16n5_osAhaYdL0j4gZMaozW3_CR","timestamp":1650176957608}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
